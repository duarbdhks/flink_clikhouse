# ë°°í¬ ê°€ì´ë“œ (Docker Compose)

## ðŸ“‹ ê°œìš”
ì „ì²´ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ Docker Composeë¡œ ë¡œì»¬ í™˜ê²½ì— ë°°í¬í•˜ëŠ” ê°€ì´ë“œ

## ðŸŽ¯ ë°°í¬ êµ¬ì„±
```
MySQL (Source DB)
    â†“
Flink CDC Job
    â†“
Kafka (KRaft Mode)
    â†“
Flink Sync Connector Job
    â†“
ClickHouse (Analytics DB)
```

## ðŸ“¦ ì‚¬ì „ ìš”êµ¬ì‚¬í•­

### 1. ì†Œí”„íŠ¸ì›¨ì–´ ì„¤ì¹˜
```bash
# Docker ë° Docker Compose ì„¤ì¹˜ í™•ì¸
docker --version       # Docker version 20.10.0+
docker-compose --version  # docker-compose version 1.29.0+

# ë©”ëª¨ë¦¬ í™•ì¸ (ìµœì†Œ 8GB ê¶Œìž¥)
free -h

# ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ (ìµœì†Œ 10GB ì—¬ìœ  ê³µê°„)
df -h
```

### 2. í”„ë¡œì íŠ¸ êµ¬ì¡°
```
flink_clickhouse/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ init-scripts/
â”‚   â”œâ”€â”€ init-mysql.sql
â”‚   â””â”€â”€ init-clickhouse.sql
â”œâ”€â”€ flink-jobs/
â”‚   â”œâ”€â”€ mysql-cdc-job.jar
â”‚   â””â”€â”€ kafka-clickhouse-sync-job.jar
â””â”€â”€ claudedocs/
    â”œâ”€â”€ pipeline/
    â””â”€â”€ infrastructure/
```

## ðŸš€ ë°°í¬ ë‹¨ê³„

### Step 1: ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸ ì¤€ë¹„

#### init-mysql.sql ìƒì„±
```bash
mkdir -p init-scripts
cat > init-scripts/init-mysql.sql << 'EOF'
-- CDC ì‚¬ìš©ìž ìƒì„±
CREATE USER 'cdc'@'%' IDENTIFIED BY 'cdc_password_123';
GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT
ON *.* TO 'cdc'@'%';
FLUSH PRIVILEGES;

-- ì£¼ë¬¸ ë°ì´í„°ë² ì´ìŠ¤ ë° í…Œì´ë¸” ìƒì„±
USE order_db;

CREATE TABLE orders (
    order_id BIGINT AUTO_INCREMENT PRIMARY KEY,
    user_id BIGINT NOT NULL,
    product_name VARCHAR(255) NOT NULL,
    quantity INT NOT NULL,
    total_price DECIMAL(10, 2) NOT NULL,
    status VARCHAR(50) NOT NULL DEFAULT 'pending',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    INDEX idx_user_id (user_id),
    INDEX idx_status (status),
    INDEX idx_created_at (created_at)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

CREATE TABLE order_items (
    item_id BIGINT AUTO_INCREMENT PRIMARY KEY,
    order_id BIGINT NOT NULL,
    product_id BIGINT NOT NULL,
    product_name VARCHAR(255) NOT NULL,
    quantity INT NOT NULL,
    unit_price DECIMAL(10, 2) NOT NULL,
    subtotal DECIMAL(10, 2) NOT NULL,
    FOREIGN KEY (order_id) REFERENCES orders(order_id) ON DELETE CASCADE,
    INDEX idx_order_id (order_id),
    INDEX idx_product_id (product_id)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

-- ìƒ˜í”Œ ë°ì´í„° ì‚½ìž…
INSERT INTO orders (user_id, product_name, quantity, total_price, status)
VALUES
    (100, 'Laptop', 1, 1500.00, 'pending'),
    (101, 'Mouse', 2, 50.00, 'completed'),
    (102, 'Keyboard', 1, 80.00, 'pending'),
    (103, 'Monitor', 1, 300.00, 'completed'),
    (104, 'Headphones', 1, 120.00, 'pending');
EOF
```

#### init-clickhouse.sql ìƒì„±
```bash
cat > init-scripts/init-clickhouse.sql << 'EOF'
CREATE DATABASE IF NOT EXISTS order_analytics;

USE order_analytics;

CREATE TABLE IF NOT EXISTS orders_realtime (
    order_id UInt64,
    user_id UInt64,
    product_name String,
    quantity UInt32,
    total_price Decimal(10, 2),
    status LowCardinality(String),
    created_at DateTime,
    updated_at DateTime,
    operation_type LowCardinality(String),
    event_timestamp UInt64,
    ingestion_time DateTime DEFAULT now()
)
ENGINE = ReplacingMergeTree(event_timestamp)
PARTITION BY toYYYYMM(created_at)
ORDER BY (order_id, event_timestamp)
SETTINGS index_granularity = 8192;

-- ì¼ë³„ ì§‘ê³„ ë·°
CREATE MATERIALIZED VIEW orders_daily_summary
ENGINE = SummingMergeTree()
PARTITION BY toYYYYMM(sale_date)
ORDER BY (sale_date, status)
AS
SELECT
    toDate(created_at) AS sale_date,
    status,
    count() AS order_count,
    sum(total_price) AS daily_revenue,
    avg(total_price) AS avg_order_value,
    uniq(user_id) AS unique_customers
FROM orders_realtime
WHERE operation_type != 'DELETE'
GROUP BY sale_date, status;

-- ì‹œê°„ëŒ€ë³„ í†µê³„ ë·°
CREATE MATERIALIZED VIEW orders_hourly_stats
ENGINE = AggregatingMergeTree()
PARTITION BY toYYYYMM(order_hour)
ORDER BY (order_hour, status)
AS
SELECT
    toStartOfHour(created_at) AS order_hour,
    status,
    countState() AS order_count,
    sumState(total_price) AS hourly_revenue,
    avgState(total_price) AS avg_order_value,
    uniqState(user_id) AS unique_customers
FROM orders_realtime
WHERE operation_type != 'DELETE'
GROUP BY order_hour, status;
EOF
```

### Step 2: Kafka Topic ìƒì„± ìŠ¤í¬ë¦½íŠ¸

```bash
cat > init-scripts/create-kafka-topics.sh << 'EOF'
#!/bin/bash

# Kafkaê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸°
sleep 10

# Topic ìƒì„±
docker exec -it kafka kafka-topics --create \
  --bootstrap-server localhost:9092 \
  --topic orders-cdc \
  --partitions 3 \
  --replication-factor 1 \
  --config retention.ms=604800000

docker exec -it kafka kafka-topics --create \
  --bootstrap-server localhost:9092 \
  --topic order-items-cdc \
  --partitions 3 \
  --replication-factor 1 \
  --config retention.ms=604800000

echo "âœ… Kafka Topics created successfully"
EOF

chmod +x init-scripts/create-kafka-topics.sh
```

### Step 3: ì¸í”„ë¼ ì‹œìž‘

#### 3.1 ì „ì²´ ì„œë¹„ìŠ¤ ì‹œìž‘
```bash
# ë°±ê·¸ë¼ìš´ë“œì—ì„œ ëª¨ë“  ì„œë¹„ìŠ¤ ì‹œìž‘
docker-compose up -d

# ë¡œê·¸ í™•ì¸
docker-compose logs -f
```

#### 3.2 ê°œë³„ ì„œë¹„ìŠ¤ ì‹œìž‘ (ì„ íƒì )
```bash
# ë°ì´í„°ë² ì´ìŠ¤ë§Œ ë¨¼ì € ì‹œìž‘
docker-compose up -d mysql clickhouse kafka

# ìƒíƒœ í™•ì¸
docker-compose ps

# Flink ë‚˜ì¤‘ì— ì‹œìž‘
docker-compose up -d flink-jobmanager flink-taskmanager
```

### Step 4: í—¬ìŠ¤ì²´í¬ í™•ì¸

```bash
# ëª¨ë“  ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
docker-compose ps

# ê°œë³„ ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬
docker exec -it mysql mysqladmin ping -h localhost
docker exec -it clickhouse-server clickhouse-client --query "SELECT 1"
docker exec -it kafka kafka-broker-api-versions --bootstrap-server localhost:9092
curl http://localhost:8081  # Flink Web UI
```

### Step 5: Kafka Topic ìƒì„±

```bash
# Topic ìƒì„± ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
./init-scripts/create-kafka-topics.sh

# Topic í™•ì¸
docker exec -it kafka kafka-topics --list --bootstrap-server localhost:9092
```

### Step 6: Flink Job ë°°í¬

#### 6.1 CDC Job ì œì¶œ
```bash
# Flink Job ë””ë ‰í† ë¦¬ì— JAR íŒŒì¼ ë³µì‚¬
cp /path/to/mysql-cdc-job.jar ./flink-jobs/

# JobManagerì— Job ì œì¶œ
docker exec -it flink-jobmanager flink run \
  -d \
  -c com.example.cdc.MySQLCDCJob \
  /opt/flink/jobs/mysql-cdc-job.jar

# Job ìƒíƒœ í™•ì¸
docker exec -it flink-jobmanager flink list
```

#### 6.2 Sync Connector Job ì œì¶œ
```bash
# Sync Connector JAR ë³µì‚¬
cp /path/to/kafka-clickhouse-sync-job.jar ./flink-jobs/

# Job ì œì¶œ
docker exec -it flink-jobmanager flink run \
  -d \
  -c com.example.sync.KafkaToClickHouseJob \
  /opt/flink/jobs/kafka-clickhouse-sync-job.jar

# Job ëª©ë¡ í™•ì¸
docker exec -it flink-jobmanager flink list
```

## ðŸ” ê²€ì¦

### 1. MySQL ë°ì´í„° í™•ì¸
```bash
docker exec -it mysql mysql -u root -p
# Password: test123

USE order_db;
SELECT * FROM orders LIMIT 5;
```

### 2. Kafka Topic ë©”ì‹œì§€ í™•ì¸
```bash
docker exec -it kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic orders-cdc \
  --from-beginning \
  --max-messages 5
```

### 3. ClickHouse ë°ì´í„° í™•ì¸
```bash
docker exec -it clickhouse-server clickhouse-client

SELECT * FROM order_analytics.orders_realtime
ORDER BY created_at DESC
LIMIT 10;
```

### 4. End-to-End í…ŒìŠ¤íŠ¸
```bash
# 1. MySQLì— ìƒˆ ì£¼ë¬¸ ì‚½ìž…
docker exec -it mysql mysql -u root -p order_db \
  -e "INSERT INTO orders (user_id, product_name, quantity, total_price) VALUES (200, 'Test Product', 1, 100.00);"

# 2. Kafkaì—ì„œ CDC ì´ë²¤íŠ¸ í™•ì¸ (1-2ì´ˆ í›„)
docker exec -it kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic orders-cdc \
  --max-messages 1

# 3. ClickHouseì—ì„œ ë°ì´í„° í™•ì¸ (3-7ì´ˆ í›„)
docker exec -it clickhouse-server clickhouse-client \
  --query "SELECT * FROM order_analytics.orders_realtime WHERE user_id = 200"
```

## ðŸ“Š ëª¨ë‹ˆí„°ë§

### Web UI ì ‘ì†
- **Flink Dashboard**: http://localhost:8081
- **Kafka UI**: http://localhost:8080
- **ClickHouse**: http://localhost:8123/play

### ë¡œê·¸ í™•ì¸
```bash
# ì „ì²´ ë¡œê·¸
docker-compose logs -f

# íŠ¹ì • ì„œë¹„ìŠ¤ ë¡œê·¸
docker-compose logs -f mysql
docker-compose logs -f kafka
docker-compose logs -f flink-jobmanager
docker-compose logs -f clickhouse
```

### ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ í™•ì¸
```bash
# ì»¨í…Œì´ë„ˆ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
docker stats

# íŠ¹ì • ì»¨í…Œì´ë„ˆ ìƒì„¸ ì •ë³´
docker inspect flink-jobmanager
```

## ðŸ›‘ ì¤‘ì§€ ë° ìž¬ì‹œìž‘

### ì„œë¹„ìŠ¤ ì¤‘ì§€
```bash
# ëª¨ë“  ì„œë¹„ìŠ¤ ì¤‘ì§€ (ë°ì´í„° ìœ ì§€)
docker-compose stop

# ëª¨ë“  ì„œë¹„ìŠ¤ ì¤‘ì§€ ë° ì œê±°
docker-compose down

# ë³¼ë¥¨ê¹Œì§€ ëª¨ë‘ ì‚­ì œ (ë°ì´í„° ì‚­ì œ)
docker-compose down -v
```

### ì„œë¹„ìŠ¤ ìž¬ì‹œìž‘
```bash
# ì „ì²´ ìž¬ì‹œìž‘
docker-compose restart

# íŠ¹ì • ì„œë¹„ìŠ¤ë§Œ ìž¬ì‹œìž‘
docker-compose restart kafka
docker-compose restart flink-jobmanager
```

## ðŸ§¹ í´ë¦°ì—…

### ì „ì²´ ì •ë¦¬
```bash
# ì»¨í…Œì´ë„ˆ, ë„¤íŠ¸ì›Œí¬, ë³¼ë¥¨ ëª¨ë‘ ì‚­ì œ
docker-compose down -v

# ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ ì •ë¦¬
docker image prune -a

# ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ë³¼ë¥¨ ì •ë¦¬
docker volume prune
```

## ðŸš¨ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: í¬íŠ¸ ì¶©ëŒ
```bash
# í¬íŠ¸ ì‚¬ìš© í™•ì¸
lsof -i :3306  # MySQL
lsof -i :9092  # Kafka
lsof -i :8123  # ClickHouse
lsof -i :8081  # Flink

# í•´ê²°: docker-compose.ymlì—ì„œ í¬íŠ¸ ë³€ê²½
# ì˜ˆ: "13306:3306" (í˜¸ìŠ¤íŠ¸:ì»¨í…Œì´ë„ˆ)
```

### ë¬¸ì œ 2: ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# Docker ë©”ëª¨ë¦¬ ì„¤ì • í™•ì¸
docker info | grep Memory

# í•´ê²°: Docker Desktop ì„¤ì •ì—ì„œ ë©”ëª¨ë¦¬ ì¦ê°€
# Docker Desktop â†’ Preferences â†’ Resources â†’ Memory: 8GB+
```

### ë¬¸ì œ 3: Flink Job ì‹¤íŒ¨
```bash
# Job ë¡œê·¸ í™•ì¸
docker-compose logs flink-jobmanager
docker-compose logs flink-taskmanager

# Job ìƒíƒœ í™•ì¸
docker exec -it flink-jobmanager flink list

# Job ì·¨ì†Œ
docker exec -it flink-jobmanager flink cancel <JOB_ID>
```

### ë¬¸ì œ 4: Kafka ì—°ê²° ì‹¤íŒ¨
```bash
# Kafka ìƒíƒœ í™•ì¸
docker exec -it kafka kafka-broker-api-versions --bootstrap-server localhost:9092

# ë„¤íŠ¸ì›Œí¬ í™•ì¸
docker network inspect flink_clickhouse_cdc-network

# í•´ê²°: ì»¨í…Œì´ë„ˆ ìž¬ì‹œìž‘
docker-compose restart kafka
```

## ðŸ“ˆ ì„±ëŠ¥ íŠœë‹

### Flink ì„±ëŠ¥ ìµœì í™”
```yaml
# docker-compose.ymlì—ì„œ TaskManager ë¦¬ì†ŒìŠ¤ ì¦ê°€
flink-taskmanager:
  environment:
    FLINK_PROPERTIES: |
      taskmanager.numberOfTaskSlots: 8
      taskmanager.memory.process.size: 2048m
  scale: 2  # TaskManager ê°œìˆ˜ ì¦ê°€
```

### Kafka ì²˜ë¦¬ëŸ‰ ì¦ê°€
```yaml
kafka:
  environment:
    KAFKA_NUM_PARTITIONS: 6  # Partition ìˆ˜ ì¦ê°€
    KAFKA_HEAP_OPTS: "-Xmx1G -Xms1G"
```

### ClickHouse ìµœì í™”
```yaml
clickhouse:
  environment:
    CLICKHOUSE_MAX_MEMORY_USAGE: 4000000000  # 4GB
```

## ðŸ” ë³´ì•ˆ ì„¤ì • (í”„ë¡œë•ì…˜ í™˜ê²½)

### ë¹„ë°€ë²ˆí˜¸ ë³€ê²½
```bash
# .env íŒŒì¼ ìƒì„±
cat > .env << EOF
MYSQL_ROOT_PASSWORD=your_secure_password
MYSQL_PASSWORD=your_app_password
CLICKHOUSE_PASSWORD=your_clickhouse_password
EOF

# docker-compose.ymlì—ì„œ í™˜ê²½ë³€ìˆ˜ ì°¸ì¡°
# ${MYSQL_ROOT_PASSWORD}
```

### ë„¤íŠ¸ì›Œí¬ ê²©ë¦¬
```yaml
networks:
  cdc-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.25.0.0/16
```

## ðŸ“š ë‹¤ìŒ ë‹¨ê³„
- [íŒŒì´í”„ë¼ì¸ E2E í…ŒìŠ¤íŠ¸](../testing/pipeline-validation.md)
- [NestJS Order Service ì„¤ì •](../order-service/api-spec.md)
- [í”„ë¡œë•ì…˜ ë°°í¬ ê°€ì´ë“œ](./production-deployment.md)
