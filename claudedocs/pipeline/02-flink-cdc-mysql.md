# Flink CDC MySQL ì„¤ì •

## ğŸ“‹ ê°œìš”
MySQL Binlogë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ìº¡ì²˜í•˜ì—¬ Kafkaë¡œ ì „ì†¡í•˜ëŠ” Flink CDC Job êµ¬ì„± ê°€ì´ë“œ

## ğŸ¯ êµ¬ì„± ìš”ì†Œ
```
MySQL (binlog enabled)
    â†“
Flink CDC Connector
    â†“
Kafka Topic (orders-cdc-topic)
```

## ğŸ“¦ í•„ìš”í•œ ì˜ì¡´ì„±

### Maven ì˜ì¡´ì„± (pom.xml)
```xml
<dependencies>
    <!-- Flink Core -->
    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-streaming-java</artifactId>
        <version>1.18.0</version>
    </dependency>

    <!-- Flink CDC MySQL Connector -->
    <dependency>
        <groupId>com.ververica</groupId>
        <artifactId>flink-connector-mysql-cdc</artifactId>
        <version>3.0.1</version>
    </dependency>

    <!-- Flink Kafka Connector -->
    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-connector-kafka</artifactId>
        <version>3.0.2-1.18</version>
    </dependency>

    <!-- JSON Serialization -->
    <dependency>
        <groupId>com.fasterxml.jackson.core</groupId>
        <artifactId>jackson-databind</artifactId>
        <version>2.15.2</version>
    </dependency>

    <!-- MySQL JDBC Driver -->
    <dependency>
        <groupId>mysql</groupId>
        <artifactId>mysql-connector-java</artifactId>
        <version>8.0.33</version>
    </dependency>
</dependencies>
```

## ğŸ”§ MySQL ì„¤ì •

### 1. Binlog í™œì„±í™” (my.cnf ë˜ëŠ” docker-compose í™˜ê²½ë³€ìˆ˜)
```ini
[mysqld]
# Binlog ì„¤ì •
server-id = 1
log_bin = mysql-bin
binlog_format = ROW
binlog_row_image = FULL
expire_logs_days = 7

# ì„ íƒì : íŠ¹ì • ë°ì´í„°ë² ì´ìŠ¤ë§Œ binlog ê¸°ë¡
# binlog-do-db = order_db
```

### 2. CDC ì „ìš© ì‚¬ìš©ì ìƒì„±
```sql
-- CDC ì‚¬ìš©ì ìƒì„±
CREATE USER 'flink_cdc'@'%' IDENTIFIED BY 'cdc_password_123';

-- í•„ìš”í•œ ê¶Œí•œ ë¶€ì—¬
GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT
ON *.* TO 'flink_cdc'@'%';

-- ê¶Œí•œ ì ìš©
FLUSH PRIVILEGES;
```

### 3. í”Œë«í¼ ë°ì´í„°ë² ì´ìŠ¤ ë° í…Œì´ë¸” ìƒì„±
```sql
CREATE DATABASE IF NOT EXISTS order_db;
USE order_db;

-- users í…Œì´ë¸”
CREATE TABLE IF NOT EXISTS users (
  id         BIGINT AUTO_INCREMENT PRIMARY KEY COMMENT 'ì‚¬ìš©ì ID',
  username   VARCHAR(50)  NOT NULL UNIQUE COMMENT 'ì‚¬ìš©ìëª… (ê³ ìœ )',
  email      VARCHAR(100) NOT NULL UNIQUE COMMENT 'ì´ë©”ì¼ (ê³ ìœ )',
  phone      VARCHAR(20) COMMENT 'ì „í™”ë²ˆí˜¸',
  created_at TIMESTAMP    NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT 'ê³„ì • ìƒì„± ì¼ì‹œ',
  INDEX idx_email (email),
  INDEX idx_username (username)
) ENGINE = InnoDB DEFAULT CHARSET = utf8mb4 COLLATE = utf8mb4_unicode_ci;

-- products í…Œì´ë¸”
CREATE TABLE IF NOT EXISTS products (
  id          BIGINT AUTO_INCREMENT PRIMARY KEY COMMENT 'ìƒí’ˆ ID',
  name        VARCHAR(255)   NOT NULL COMMENT 'ìƒí’ˆëª…',
  category    VARCHAR(50) COMMENT 'ì¹´í…Œê³ ë¦¬',
  price       DECIMAL(10, 2) NOT NULL COMMENT 'íŒë§¤ ê°€ê²©',
  stock       INT            NOT NULL DEFAULT 0 COMMENT 'ì¬ê³  ìˆ˜ëŸ‰',
  description TEXT COMMENT 'ìƒí’ˆ ì„¤ëª…',
  created_at  TIMESTAMP      NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT 'ìƒí’ˆ ë“±ë¡ ì¼ì‹œ',
  updated_at  TIMESTAMP      NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT 'ë§ˆì§€ë§‰ ìˆ˜ì • ì¼ì‹œ',
  INDEX idx_category (category),
  INDEX idx_price (price),
  INDEX idx_name (name)
) ENGINE = InnoDB DEFAULT CHARSET = utf8mb4 COLLATE = utf8mb4_unicode_ci;

-- orders í…Œì´ë¸”
CREATE TABLE IF NOT EXISTS orders (
  id           BIGINT AUTO_INCREMENT PRIMARY KEY COMMENT 'ì£¼ë¬¸ ID',
  user_id      BIGINT         NOT NULL COMMENT 'ì‚¬ìš©ì ID',
  status       VARCHAR(20)    NOT NULL DEFAULT 'PENDING' COMMENT 'ì£¼ë¬¸ ìƒíƒœ',
  total_amount DECIMAL(10, 2) NOT NULL DEFAULT 0.00 COMMENT 'ì´ ì£¼ë¬¸ ê¸ˆì•¡',
  order_date   TIMESTAMP      NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT 'ì£¼ë¬¸ ìƒì„± ì¼ì‹œ',
  updated_at   TIMESTAMP      NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT 'ë§ˆì§€ë§‰ ìˆ˜ì • ì¼ì‹œ',
  INDEX idx_user_id (user_id),
  INDEX idx_status (status),
  INDEX idx_order_date (order_date)
) ENGINE = InnoDB DEFAULT CHARSET = utf8mb4 COLLATE = utf8mb4_unicode_ci;

-- order_items í…Œì´ë¸”
CREATE TABLE IF NOT EXISTS order_items (
  id           BIGINT AUTO_INCREMENT PRIMARY KEY COMMENT 'ì£¼ë¬¸ í•­ëª© ID',
  order_id     BIGINT         NOT NULL COMMENT 'ì£¼ë¬¸ ID (FK)',
  product_id   BIGINT         NOT NULL COMMENT 'ìƒí’ˆ ID',
  product_name VARCHAR(255)   NOT NULL COMMENT 'ìƒí’ˆëª…',
  quantity     INT            NOT NULL DEFAULT 1 COMMENT 'ìˆ˜ëŸ‰',
  price        DECIMAL(10, 2) NOT NULL COMMENT 'ë‹¨ê°€',
  subtotal     DECIMAL(10, 2) NOT NULL COMMENT 'ì†Œê³„',
  created_at   TIMESTAMP      NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT 'ìƒì„± ì¼ì‹œ',
  FOREIGN KEY (order_id) REFERENCES orders (id) ON DELETE CASCADE,
  INDEX idx_order_id (order_id),
  INDEX idx_product_id (product_id)
) ENGINE = InnoDB DEFAULT CHARSET = utf8mb4 COLLATE = utf8mb4_unicode_ci;
```

## ğŸš€ Flink CDC Job êµ¬í˜„

### Job êµ¬ì¡°
```
src/main/java/com/example/cdc/
â”œâ”€â”€ MySQLCDCJob.java              (ë©”ì¸ Job)
â”œâ”€â”€ config/
â”‚   â””â”€â”€ CDCConfig.java            (ì„¤ì • í´ë˜ìŠ¤)
â”œâ”€â”€ serializer/
â”‚   â””â”€â”€ OrderEventSerializer.java (Kafka ì§ë ¬í™”)
â””â”€â”€ model/
    â””â”€â”€ OrderEvent.java           (ë°ì´í„° ëª¨ë¸)
```

### 1. ë°ì´í„° ëª¨ë¸ (OrderEvent.java)
```java
package com.example.cdc.model;

import com.fasterxml.jackson.annotation.JsonProperty;
import java.math.BigDecimal;
import java.time.LocalDateTime;

public class OrderEvent {
    @JsonProperty("order_id")
    private Long orderId;

    @JsonProperty("user_id")
    private Long userId;

    @JsonProperty("product_name")
    private String productName;

    @JsonProperty("quantity")
    private Integer quantity;

    @JsonProperty("total_price")
    private BigDecimal totalPrice;

    @JsonProperty("status")
    private String status;

    @JsonProperty("created_at")
    private LocalDateTime createdAt;

    @JsonProperty("updated_at")
    private LocalDateTime updatedAt;

    @JsonProperty("operation")
    private String operation; // INSERT, UPDATE, DELETE

    @JsonProperty("event_timestamp")
    private Long eventTimestamp;

    // Getters and Setters
    // ... (ìƒëµ)
}
```

### 2. CDC ì„¤ì • í´ë˜ìŠ¤ (CDCConfig.java)
```java
package com.example.cdc.config;

public class CDCConfig {
    // MySQL ì„¤ì •
    public static final String MYSQL_HOST = System.getenv().getOrDefault("MYSQL_HOST", "mysql");
    public static final int MYSQL_PORT = Integer.parseInt(System.getenv().getOrDefault("MYSQL_PORT", "3306"));
    public static final String MYSQL_DATABASE = System.getenv().getOrDefault("MYSQL_DATABASE", "order_db");
    public static final String MYSQL_USERNAME = System.getenv().getOrDefault("MYSQL_USERNAME", "flink_cdc");
    public static final String MYSQL_PASSWORD = System.getenv().getOrDefault("MYSQL_PASSWORD", "cdc_password_123");

    // CDC ì„¤ì •
    public static final String SERVER_ID = System.getenv().getOrDefault("CDC_SERVER_ID", "5400-5404");
    public static final String[] TABLES = {
        MYSQL_DATABASE + ".orders",
        MYSQL_DATABASE + ".order_items"
    };

    // Kafka ì„¤ì •
    public static final String KAFKA_BOOTSTRAP_SERVERS = System.getenv().getOrDefault(
        "KAFKA_BOOTSTRAP_SERVERS", "kafka:9092"
    );
    public static final String KAFKA_TOPIC_ORDERS = "orders-cdc-topic";
    public static final String KAFKA_TOPIC_ORDER_ITEMS = "order-items-cdc-topic";

    // Checkpoint ì„¤ì •
    public static final long CHECKPOINT_INTERVAL = 60000L; // 1ë¶„
}
```

### 3. ë©”ì¸ CDC Job (MySQLCDCJob.java)
```java
package com.example.cdc;

import com.example.cdc.config.CDCConfig;
import com.example.cdc.model.OrderEvent;
import com.ververica.cdc.connectors.mysql.source.MySqlSource;
import com.ververica.cdc.connectors.mysql.table.StartupOptions;
import com.ververica.cdc.debezium.JsonDebeziumDeserializationSchema;
import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.common.serialization.SimpleStringSchema;
import org.apache.flink.connector.kafka.sink.KafkaRecordSerializationSchema;
import org.apache.flink.connector.kafka.sink.KafkaSink;
import org.apache.flink.streaming.api.datastream.DataStreamSource;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.ProcessFunction;
import org.apache.flink.util.Collector;

import java.util.Properties;

public class MySQLCDCJob {
    public static void main(String[] args) throws Exception {
        // 1. Flink ì‹¤í–‰ í™˜ê²½ ì„¤ì •
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // 2. Checkpoint ì„¤ì • (Exactly-Once ë³´ì¥)
        env.enableCheckpointing(CDCConfig.CHECKPOINT_INTERVAL);
        env.getCheckpointConfig().setCheckpointStorage("file:///tmp/flink-checkpoints");

        // 3. MySQL CDC Source ìƒì„±
        MySqlSource<String> mySqlSource = MySqlSource.<String>builder()
            .hostname(CDCConfig.MYSQL_HOST)
            .port(CDCConfig.MYSQL_PORT)
            .databaseList(CDCConfig.MYSQL_DATABASE)
            .tableList(CDCConfig.TABLES)
            .username(CDCConfig.MYSQL_USERNAME)
            .password(CDCConfig.MYSQL_PASSWORD)
            .serverId(CDCConfig.SERVER_ID)
            .startupOptions(StartupOptions.initial()) // ì´ˆê¸° ìŠ¤ëƒ…ìƒ· + ì¦ë¶„ ë™ê¸°í™”
            .deserializer(new JsonDebeziumDeserializationSchema()) // JSON í˜•ì‹ìœ¼ë¡œ ì—­ì§ë ¬í™”
            .build();

        // 4. CDC ìŠ¤íŠ¸ë¦¼ ìƒì„±
        DataStreamSource<String> cdcStream = env
            .fromSource(mySqlSource, WatermarkStrategy.noWatermarks(), "MySQL CDC Source");

        // 5. í…Œì´ë¸”ë³„ ë¼ìš°íŒ… ë° Kafkaë¡œ ì „ì†¡
        cdcStream
            .process(new ProcessFunction<String, String>() {
                @Override
                public void processElement(String value, Context ctx, Collector<String> out) throws Exception {
                    // JSON íŒŒì‹±í•˜ì—¬ í…Œì´ë¸”ëª… ì¶”ì¶œ
                    if (value.contains("\"table\":\"orders\"")) {
                        ctx.output(ordersSideOutput, value);
                    } else if (value.contains("\"table\":\"order_items\"")) {
                        ctx.output(orderItemsSideOutput, value);
                    }
                }
            });

        // 6. Kafka Sink ìƒì„± (orders í…Œì´ë¸”)
        KafkaSink<String> ordersSink = KafkaSink.<String>builder()
            .setBootstrapServers(CDCConfig.KAFKA_BOOTSTRAP_SERVERS)
            .setRecordSerializer(
                KafkaRecordSerializationSchema.builder()
                    .setTopic(CDCConfig.KAFKA_TOPIC_ORDERS)
                    .setValueSerializationSchema(new SimpleStringSchema())
                    .build()
            )
            .build();

        // 7. Kafka Sink ìƒì„± (order_items í…Œì´ë¸”)
        KafkaSink<String> orderItemsSink = KafkaSink.<String>builder()
            .setBootstrapServers(CDCConfig.KAFKA_BOOTSTRAP_SERVERS)
            .setRecordSerializer(
                KafkaRecordSerializationSchema.builder()
                    .setTopic(CDCConfig.KAFKA_TOPIC_ORDER_ITEMS)
                    .setValueSerializationSchema(new SimpleStringSchema())
                    .build()
            )
            .build();

        // 8. Kafkaë¡œ ì „ì†¡
        cdcStream
            .filter(value -> value.contains("\"table\":\"orders\""))
            .sinkTo(ordersSink)
            .name("Orders CDC to Kafka");

        cdcStream
            .filter(value -> value.contains("\"table\":\"order_items\""))
            .sinkTo(orderItemsSink)
            .name("Order Items CDC to Kafka");

        // 9. Job ì‹¤í–‰
        env.execute("MySQL CDC to Kafka Job");
    }
}
```

### 4. ê°„ì†Œí™”ëœ ë²„ì „ (ë‹¨ì¼ Kafka Topic)
```java
package com.example.cdc;

import com.example.cdc.config.CDCConfig;
import com.ververica.cdc.connectors.mysql.source.MySqlSource;
import com.ververica.cdc.connectors.mysql.table.StartupOptions;
import com.ververica.cdc.debezium.JsonDebeziumDeserializationSchema;
import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.common.serialization.SimpleStringSchema;
import org.apache.flink.connector.kafka.sink.KafkaRecordSerializationSchema;
import org.apache.flink.connector.kafka.sink.KafkaSink;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;

public class MySQLCDCJobSimple {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.enableCheckpointing(60000);

        // MySQL CDC Source
        MySqlSource<String> mySqlSource = MySqlSource.<String>builder()
            .hostname("mysql")
            .port(3306)
            .databaseList("order_db")
            .tableList("order_db.orders", "order_db.order_items")
            .username("flink_cdc")
            .password("cdc_password_123")
            .startupOptions(StartupOptions.initial())
            .deserializer(new JsonDebeziumDeserializationSchema())
            .build();

        // Kafka Sink
        KafkaSink<String> kafkaSink = KafkaSink.<String>builder()
            .setBootstrapServers("kafka:9092")
            .setRecordSerializer(
                KafkaRecordSerializationSchema.builder()
                    .setTopic("orders-cdc-topic")
                    .setValueSerializationSchema(new SimpleStringSchema())
                    .build()
            )
            .build();

        // Pipeline
        env.fromSource(mySqlSource, WatermarkStrategy.noWatermarks(), "MySQL CDC")
            .sinkTo(kafkaSink)
            .name("CDC to Kafka");

        env.execute("MySQL CDC Job");
    }
}
```

## ğŸ“Š CDC ì´ë²¤íŠ¸ í˜•ì‹

### Flink CDC Change Event êµ¬ì¡°
```json
{
  "before": null,
  "after": {
    "order_id": 1001,
    "user_id": 500,
    "product_name": "Laptop",
    "quantity": 2,
    "total_price": 2000.00,
    "status": "pending",
    "created_at": "2025-01-11T10:30:00Z",
    "updated_at": "2025-01-11T10:30:00Z"
  },
  "source": {
    "version": "3.0.1",
    "connector": "mysql",
    "name": "mysql-server",
    "ts_ms": 1736592600000,
    "snapshot": "false",
    "db": "order_db",
    "table": "orders",
    "server_id": 1,
    "gtid": null,
    "file": "mysql-bin.000003",
    "pos": 1234,
    "row": 0
  },
  "op": "c",  // c=create, u=update, d=delete, r=read(snapshot)
  "ts_ms": 1736592600123,
  "transaction": null
}
```

### ì‘ì—… ìœ í˜• (op í•„ë“œ)
- **`c` (create)**: INSERT ì‘ì—…
- **`u` (update)**: UPDATE ì‘ì—…
- **`d` (delete)**: DELETE ì‘ì—…
- **`r` (read)**: ì´ˆê¸° ìŠ¤ëƒ…ìƒ· ì½ê¸°

## ğŸ” ëª¨ë‹ˆí„°ë§ ë° ë””ë²„ê¹…

### 1. Flink Web UIì—ì„œ í™•ì¸
```
http://localhost:8081

í™•ì¸ í•­ëª©:
- Job ìƒíƒœ (RUNNING)
- Checkpoints ì„±ê³µë¥ 
- Records Sent (Kafkaë¡œ ì „ì†¡ëœ ë ˆì½”ë“œ ìˆ˜)
- Backpressure (ì—­ì•• ìƒíƒœ)
```

### 2. Kafka Topic í™•ì¸
```bash
# ì»¨í…Œì´ë„ˆ ì ‘ì†
docker exec -it kafka bash

# Topic ëª©ë¡ í™•ì¸
kafka-topics --bootstrap-server localhost:9092 --list

# CDC ì´ë²¤íŠ¸ í™•ì¸
kafka-console-consumer --bootstrap-server localhost:9092 \
  --topic orders-cdc-topic \
  --from-beginning \
  --max-messages 10
```

### 3. MySQL Binlog ìƒíƒœ í™•ì¸
```sql
-- Binlog í™œì„±í™” í™•ì¸
SHOW VARIABLES LIKE 'log_bin';

-- Binlog íŒŒì¼ ëª©ë¡
SHOW BINARY LOGS;

-- í˜„ì¬ Binlog ìœ„ì¹˜
SHOW MASTER STATUS;

-- Binlog ì´ë²¤íŠ¸ í™•ì¸
SHOW BINLOG EVENTS IN 'mysql-bin.000003' LIMIT 10;
```

## âš™ï¸ Startup Options

### 1. initial() - ì´ˆê¸° ìŠ¤ëƒ…ìƒ· + ì¦ë¶„ ë™ê¸°í™” (ê¶Œì¥)
```java
.startupOptions(StartupOptions.initial())
```
- ê¸°ì¡´ ë°ì´í„° ì „ì²´ ìŠ¤ëƒ…ìƒ· í›„ binlog ì¦ë¶„ ë™ê¸°í™”
- **MVP í…ŒìŠ¤íŠ¸ì— ì í•©**

### 2. latest() - ìµœì‹  binlogë¶€í„°
```java
.startupOptions(StartupOptions.latest())
```
- Job ì‹œì‘ ì´í›„ì˜ ë³€ê²½ì‚¬í•­ë§Œ ìº¡ì²˜
- ê¸°ì¡´ ë°ì´í„° ë¬´ì‹œ

### 3. timestamp() - íŠ¹ì • ì‹œì ë¶€í„°
```java
.startupOptions(StartupOptions.timestamp(1736592600000L))
```
- íŠ¹ì • íƒ€ì„ìŠ¤íƒ¬í”„ ì´í›„ì˜ ë³€ê²½ì‚¬í•­ ìº¡ì²˜

### 4. specificOffset() - íŠ¹ì • binlog ìœ„ì¹˜ë¶€í„°
```java
.startupOptions(StartupOptions.specificOffset("mysql-bin.000003", 1234))
```
- íŠ¹ì • binlog íŒŒì¼ ë° offsetë¶€í„° ì‹œì‘

## ğŸ§ª í…ŒìŠ¤íŠ¸ ë°©ë²•

### 1. MySQLì— í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚½ì…
```sql
USE order_db;

-- ì£¼ë¬¸ ìƒì„± (order_itemsëŠ” ë³„ë„ë¡œ ì¶”ê°€)
INSERT INTO orders (user_id, status, total_amount, order_date)
VALUES (101, 'PENDING', 500.00, NOW());

-- ì£¼ë¬¸ í•­ëª© ì¶”ê°€
INSERT INTO order_items (order_id, product_id, product_name, quantity, price, subtotal)
VALUES (LAST_INSERT_ID(), 1001, 'Test Product', 5, 100.00, 500.00);
```

### 2. Kafkaì—ì„œ ì´ë²¤íŠ¸ í™•ì¸
```bash
kafka-console-consumer --bootstrap-server kafka:9092 \
  --topic orders-cdc-topic \
  --from-beginning
```

### 3. UPDATE í…ŒìŠ¤íŠ¸
```sql
UPDATE orders SET status = 'completed' WHERE order_id = 1001;
```

### 4. DELETE í…ŒìŠ¤íŠ¸
```sql
DELETE FROM orders WHERE order_id = 1001;
```

## ğŸš¨ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: CDC Jobì´ ì‹œì‘ë˜ì§€ ì•ŠìŒ
```
ì›ì¸: MySQL binlogê°€ ë¹„í™œì„±í™”ë¨
í•´ê²°: my.cnfì—ì„œ log_bin ì„¤ì • í™•ì¸
```

### ë¬¸ì œ 2: ê¶Œí•œ ì˜¤ë¥˜ (Access denied)
```sql
-- ê¶Œí•œ ì¬ë¶€ì—¬
GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT
ON *.* TO 'flink_cdc'@'%';
FLUSH PRIVILEGES;
```

### ë¬¸ì œ 3: Checkpoint ì‹¤íŒ¨
```
ì›ì¸: Checkpoint ì €ì¥ ê²½ë¡œ ë¬¸ì œ
í•´ê²°:
env.getCheckpointConfig().setCheckpointStorage("file:///tmp/flink-checkpoints");
# ë˜ëŠ” HDFS, S3 ì‚¬ìš©
```

### ë¬¸ì œ 4: Kafka ì—°ê²° ì‹¤íŒ¨
```
ì›ì¸: Kafka ë¶€íŠ¸ìŠ¤íŠ¸ë© ì„œë²„ ì£¼ì†Œ ì˜¤ë¥˜
í•´ê²°: Docker ë„¤íŠ¸ì›Œí¬ì—ì„œ 'kafka' í˜¸ìŠ¤íŠ¸ëª… ì‚¬ìš©
```

## ğŸ“¦ Docker ë°°í¬ ì„¤ì •

### Dockerfile (Flink Job)
```dockerfile
FROM flink:1.18.0-scala_2.12-java11

# CDC Job JAR ë³µì‚¬
COPY target/mysql-cdc-job.jar /opt/flink/usrlib/

# MySQL Connector ë³µì‚¬ (ëŸ°íƒ€ì„ ì˜ì¡´ì„±)
COPY lib/mysql-connector-java-8.0.33.jar /opt/flink/lib/
COPY lib/flink-connector-mysql-cdc-3.0.1.jar /opt/flink/lib/
COPY lib/flink-connector-kafka-3.0.2-1.18.jar /opt/flink/lib/

# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
ENV MYSQL_HOST=mysql
ENV KAFKA_BOOTSTRAP_SERVERS=kafka:9092
```

## ğŸ” ë³´ì•ˆ ê³ ë ¤ì‚¬í•­

### 1. CDC ì‚¬ìš©ì ê¶Œí•œ ìµœì†Œí™”
```sql
-- íŠ¹ì • ë°ì´í„°ë² ì´ìŠ¤ë§Œ ì ‘ê·¼
CREATE USER 'flink_cdc'@'%' IDENTIFIED BY 'strong_password';
GRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT ON order_db.* TO 'flink_cdc'@'%';
```

### 2. ë¹„ë°€ë²ˆí˜¸ ì•”í˜¸í™”
```bash
# í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬
export MYSQL_PASSWORD=$(echo "cdc_password_123" | base64)
```

### 3. SSL ì—°ê²° í™œì„±í™”
```java
.jdbcProperties(Map.of(
    "useSSL", "true",
    "requireSSL", "true"
))
```

## ğŸ“š ë‹¤ìŒ ë‹¨ê³„
- [Confluent Kafka êµ¬ì„±](./03-confluent-kafka.md) - CDC ì´ë²¤íŠ¸ë¥¼ ë°›ì„ Kafka ì„¤ì •
- [Flink Sync Connector](./04-flink-sync-connector.md) - Kafka â†’ ClickHouse ë™ê¸°í™”
