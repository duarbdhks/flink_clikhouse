# Flink Sync Connector ì„¤ì • (Kafka â†’ ClickHouse)

## ğŸ“‹ ê°œìš”
Kafkaì—ì„œ CDC ì´ë²¤íŠ¸ë¥¼ ì†Œë¹„í•˜ì—¬ ClickHouseë¡œ ì‹¤ì‹œê°„ ë™ê¸°í™”í•˜ëŠ” Flink Job êµ¬ì„±

## ğŸ¯ ë°ì´í„° íë¦„
```
Kafka Topic (orders-cdc-topic)
    â†“
Flink Kafka Consumer
    â†“
Data Transformation & Enrichment
    â†“
Batch Buffering (ì„±ëŠ¥ ìµœì í™”)
    â†“
ClickHouse Sink (Batch Insert)
    â†“
ClickHouse Table (orders_realtime)
```

## ğŸ“¦ í•„ìš”í•œ ì˜ì¡´ì„±

### Maven ì˜ì¡´ì„± (pom.xml)
```xml
<dependencies>
    <!-- Flink Core -->
    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-streaming-java</artifactId>
        <version>1.18.0</version>
    </dependency>

    <!-- Flink Kafka Connector -->
    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-connector-kafka</artifactId>
        <version>3.0.2-1.18</version>
    </dependency>

    <!-- ClickHouse ê³µì‹ Flink Connector (Native Sink) â­ -->
    <dependency>
        <groupId>com.alibaba.ververica</groupId>
        <artifactId>flink-connector-clickhouse</artifactId>
        <version>1.2.0</version>
    </dependency>

    <!-- JSON Processing -->
    <dependency>
        <groupId>com.fasterxml.jackson.core</groupId>
        <artifactId>jackson-databind</artifactId>
        <version>2.15.2</version>
    </dependency>

    <!-- Lombok (Optional) -->
    <dependency>
        <groupId>org.projectlombok</groupId>
        <artifactId>lombok</artifactId>
        <version>1.18.30</version>
        <scope>provided</scope>
    </dependency>
</dependencies>
```

### Connector ì„ íƒ ì´ìœ 
**ClickHouse Native Sink (flink-connector-clickhouse) ì¥ì **:
- âœ… **ClickHouse ë„¤ì´í‹°ë¸Œ í”„ë¡œí† ì½œ ì‚¬ìš©** - JDBCë³´ë‹¤ 2-3ë°° ë¹ ë¦„
- âœ… **ìë™ Batch ìµœì í™”** - ClickHouseì— ìµœì í™”ëœ ë°°ì¹˜ ì²˜ë¦¬
- âœ… **Exactly-Once ë³´ì¥** - ë¶„ì‚° íŠ¸ëœì­ì…˜ ì§€ì›
- âœ… **ë°±í”„ë ˆì…” ì²˜ë¦¬** - ìë™ìœ¼ë¡œ ë¶€í•˜ ì¡°ì ˆ
- âœ… **ì—ëŸ¬ í•¸ë“¤ë§** - ì¬ì‹œë„ ë° ë°ë“œë ˆí„° í ì§€ì›

## ğŸ—ï¸ Job êµ¬ì¡°
```
src/main/java/com/example/sync/
â”œâ”€â”€ KafkaToClickHouseJob.java         (ë©”ì¸ Job)
â”œâ”€â”€ config/
â”‚   â””â”€â”€ SyncConfig.java               (ì„¤ì • í´ë˜ìŠ¤)
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ CDCEvent.java                 (CDC ì´ë²¤íŠ¸ ëª¨ë¸)
â”‚   â””â”€â”€ OrderRecord.java              (ClickHouse ë ˆì½”ë“œ)
â”œâ”€â”€ deserializer/
â”‚   â””â”€â”€ CDCEventDeserializer.java     (Kafka ì—­ì§ë ¬í™”)
â””â”€â”€ transformer/
    â””â”€â”€ CDCToClickHouseTransformer.java (ë°ì´í„° ë³€í™˜)
```

## ğŸ”§ êµ¬í˜„

### 1. CDC ì´ë²¤íŠ¸ ëª¨ë¸ (CDCEvent.java)
```java
package com.example.sync.model;

import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import com.fasterxml.jackson.annotation.JsonProperty;
import lombok.Data;

import java.util.Map;

@Data
@JsonIgnoreProperties(ignoreUnknown = true)
public class CDCEvent {
    @JsonProperty("before")
    private Map<String, Object> before;

    @JsonProperty("after")
    private Map<String, Object> after;

    @JsonProperty("source")
    private SourceInfo source;

    @JsonProperty("op")
    private String operation;  // c, u, d, r

    @JsonProperty("ts_ms")
    private Long eventTimestamp;

    @Data
    @JsonIgnoreProperties(ignoreUnknown = true)
    public static class SourceInfo {
        @JsonProperty("db")
        private String database;

        @JsonProperty("table")
        private String table;

        @JsonProperty("ts_ms")
        private Long sourceTimestamp;

        @JsonProperty("file")
        private String binlogFile;

        @JsonProperty("pos")
        private Long binlogPos;
    }

    public boolean isInsert() {
        return "c".equals(operation) || "r".equals(operation);
    }

    public boolean isUpdate() {
        return "u".equals(operation);
    }

    public boolean isDelete() {
        return "d".equals(operation);
    }
}
```

### 2. ClickHouse ë ˆì½”ë“œ ëª¨ë¸ (OrderRecord.java)
```java
package com.example.sync.model;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.math.BigDecimal;
import java.time.LocalDateTime;

@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class OrderRecord {
    private Long orderId;
    private Long userId;
    private String productName;
    private Integer quantity;
    private BigDecimal totalPrice;
    private String status;
    private LocalDateTime createdAt;
    private LocalDateTime updatedAt;
    private String operationType;  // INSERT, UPDATE, DELETE
    private Long eventTimestamp;
}
```

### 3. ì„¤ì • í´ë˜ìŠ¤ (SyncConfig.java)
```java
package com.example.sync.config;

public class SyncConfig {
    // Kafka ì„¤ì •
    public static final String KAFKA_BOOTSTRAP_SERVERS = System.getenv()
        .getOrDefault("KAFKA_BOOTSTRAP_SERVERS", "kafka:9092");
    public static final String KAFKA_TOPIC_ORDERS = "orders-cdc-topic";
    public static final String KAFKA_GROUP_ID = "flink-sync-connector";

    // ClickHouse ì„¤ì •
    public static final String CLICKHOUSE_URL = System.getenv()
        .getOrDefault("CLICKHOUSE_URL", "jdbc:clickhouse://clickhouse:8123/order_analytics");
    public static final String CLICKHOUSE_USERNAME = System.getenv()
        .getOrDefault("CLICKHOUSE_USERNAME", "default");
    public static final String CLICKHOUSE_PASSWORD = System.getenv()
        .getOrDefault("CLICKHOUSE_PASSWORD", "");

    // Batch ì„¤ì •
    public static final int BATCH_SIZE = Integer.parseInt(
        System.getenv().getOrDefault("BATCH_SIZE", "1000")
    );
    public static final long BATCH_INTERVAL_MS = Long.parseLong(
        System.getenv().getOrDefault("BATCH_INTERVAL_MS", "5000")
    );

    // Checkpoint ì„¤ì •
    public static final long CHECKPOINT_INTERVAL = 60000L; // 1ë¶„
}
```

### 4. CDC â†’ ClickHouse ë³€í™˜ (CDCToClickHouseTransformer.java)
```java
package com.example.sync.transformer;

import com.example.sync.model.CDCEvent;
import com.example.sync.model.OrderRecord;
import org.apache.flink.api.common.functions.MapFunction;

import java.math.BigDecimal;
import java.time.Instant;
import java.time.LocalDateTime;
import java.time.ZoneId;
import java.util.Map;

public class CDCToClickHouseTransformer implements MapFunction<CDCEvent, OrderRecord> {

    @Override
    public OrderRecord map(CDCEvent event) throws Exception {
        Map<String, Object> data;
        String operationType;

        if (event.isDelete()) {
            data = event.getBefore();
            operationType = "DELETE";
        } else {
            data = event.getAfter();
            operationType = event.isInsert() ? "INSERT" : "UPDATE";
        }

        if (data == null) {
            return null;
        }

        return OrderRecord.builder()
            .orderId(getLong(data, "order_id"))
            .userId(getLong(data, "user_id"))
            .productName(getString(data, "product_name"))
            .quantity(getInteger(data, "quantity"))
            .totalPrice(getBigDecimal(data, "total_price"))
            .status(getString(data, "status"))
            .createdAt(getLocalDateTime(data, "created_at"))
            .updatedAt(getLocalDateTime(data, "updated_at"))
            .operationType(operationType)
            .eventTimestamp(event.getEventTimestamp())
            .build();
    }

    private Long getLong(Map<String, Object> data, String key) {
        Object value = data.get(key);
        if (value == null) return null;
        return value instanceof Number ? ((Number) value).longValue() : Long.parseLong(value.toString());
    }

    private Integer getInteger(Map<String, Object> data, String key) {
        Object value = data.get(key);
        if (value == null) return null;
        return value instanceof Number ? ((Number) value).intValue() : Integer.parseInt(value.toString());
    }

    private String getString(Map<String, Object> data, String key) {
        Object value = data.get(key);
        return value != null ? value.toString() : null;
    }

    private BigDecimal getBigDecimal(Map<String, Object> data, String key) {
        Object value = data.get(key);
        if (value == null) return null;
        return value instanceof BigDecimal ? (BigDecimal) value : new BigDecimal(value.toString());
    }

    private LocalDateTime getLocalDateTime(Map<String, Object> data, String key) {
        Object value = data.get(key);
        if (value == null) return null;

        // MySQL timestampëŠ” ë°€ë¦¬ì´ˆ ë‹¨ìœ„
        if (value instanceof Number) {
            long epochMilli = ((Number) value).longValue();
            return LocalDateTime.ofInstant(Instant.ofEpochMilli(epochMilli), ZoneId.systemDefault());
        }

        // ISO 8601 ë¬¸ìì—´ íŒŒì‹±
        return LocalDateTime.parse(value.toString().replace("Z", ""));
    }
}
```

### 5. ë©”ì¸ Sync Job - ClickHouse Native Sink ì‚¬ìš© (KafkaToClickHouseJob.java)
```java
package com.example.sync;

import com.alibaba.ververica.cdc.connectors.clickhouse.ClickHouseSink;
import com.alibaba.ververica.cdc.connectors.clickhouse.ClickHouseSinkFunction;
import com.alibaba.ververica.cdc.connectors.clickhouse.config.ClickHouseSinkConfig;
import com.example.sync.config.SyncConfig;
import com.example.sync.model.CDCEvent;
import com.example.sync.model.OrderRecord;
import com.example.sync.transformer.CDCToClickHouseTransformer;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.json.JsonMapper;
import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.common.serialization.DeserializationSchema;
import org.apache.flink.api.common.typeinfo.TypeInformation;
import org.apache.flink.connector.kafka.source.KafkaSource;
import org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializer;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;

import java.io.IOException;
import java.sql.Timestamp;
import java.util.Properties;

public class KafkaToClickHouseJob {
    private static final ObjectMapper objectMapper = JsonMapper.builder().build();

    public static void main(String[] args) throws Exception {
        // 1. Flink ì‹¤í–‰ í™˜ê²½
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.enableCheckpointing(SyncConfig.CHECKPOINT_INTERVAL);

        // 2. Kafka Source ìƒì„±
        KafkaSource<CDCEvent> kafkaSource = KafkaSource.<CDCEvent>builder()
            .setBootstrapServers(SyncConfig.KAFKA_BOOTSTRAP_SERVERS)
            .setTopics(SyncConfig.KAFKA_TOPIC_ORDERS)
            .setGroupId(SyncConfig.KAFKA_GROUP_ID)
            .setStartingOffsets(OffsetsInitializer.earliest())
            .setValueOnlyDeserializer(new DeserializationSchema<CDCEvent>() {
                @Override
                public CDCEvent deserialize(byte[] message) throws IOException {
                    return objectMapper.readValue(message, CDCEvent.class);
                }

                @Override
                public boolean isEndOfStream(CDCEvent nextElement) {
                    return false;
                }

                @Override
                public TypeInformation<CDCEvent> getProducedType() {
                    return TypeInformation.of(CDCEvent.class);
                }
            })
            .build();

        // 3. Kafka ìŠ¤íŠ¸ë¦¼ ìƒì„±
        DataStream<CDCEvent> cdcStream = env
            .fromSource(kafkaSource, WatermarkStrategy.noWatermarks(), "Kafka CDC Source");

        // 4. CDC ì´ë²¤íŠ¸ â†’ ClickHouse ë ˆì½”ë“œ ë³€í™˜
        DataStream<OrderRecord> orderStream = cdcStream
            .map(new CDCToClickHouseTransformer())
            .filter(record -> record != null)
            .name("Transform CDC to ClickHouse");

        // 5. ClickHouse Native Sink ì„¤ì • â­
        Properties clickHouseProps = new Properties();
        clickHouseProps.setProperty("clickhouse.hosts", "clickhouse:8123");
        clickHouseProps.setProperty("clickhouse.username", SyncConfig.CLICKHOUSE_USERNAME);
        clickHouseProps.setProperty("clickhouse.password", SyncConfig.CLICKHOUSE_PASSWORD);
        clickHouseProps.setProperty("clickhouse.database", "order_analytics");
        clickHouseProps.setProperty("clickhouse.table", "orders_realtime");

        // Batch ì„¤ì • (ClickHouse ìµœì í™”)
        clickHouseProps.setProperty("clickhouse.batch.size", String.valueOf(SyncConfig.BATCH_SIZE));
        clickHouseProps.setProperty("clickhouse.batch.interval.ms", String.valueOf(SyncConfig.BATCH_INTERVAL_MS));
        clickHouseProps.setProperty("clickhouse.max.retries", "3");
        clickHouseProps.setProperty("clickhouse.ignore-delete", "false");  // DELETE ì´ë²¤íŠ¸ë„ ì²˜ë¦¬

        // 6. ClickHouse Sink Function ìƒì„±
        ClickHouseSinkFunction<OrderRecord> sinkFunction = new ClickHouseSinkFunction<>(
            clickHouseProps,
            record -> new Object[]{
                record.getOrderId(),
                record.getUserId(),
                record.getProductName(),
                record.getQuantity(),
                record.getTotalPrice(),
                record.getStatus(),
                Timestamp.valueOf(record.getCreatedAt()),
                Timestamp.valueOf(record.getUpdatedAt()),
                record.getOperationType(),
                record.getEventTimestamp()
            }
        );

        // 7. Sink ì—°ê²°
        orderStream
            .addSink(ClickHouseSink.sink(sinkFunction))
            .name("ClickHouse Native Sink");

        // 8. Job ì‹¤í–‰
        env.execute("Kafka to ClickHouse Sync Job");
    }
}
```

### 6. ê°„ì†Œí™”ëœ ë²„ì „ (ClickHouseSinkBuilder ì‚¬ìš©)
```java
package com.example.sync;

import com.alibaba.ververica.cdc.connectors.clickhouse.ClickHouseSinkBuilder;
import com.example.sync.config.SyncConfig;
import com.example.sync.model.CDCEvent;
import com.example.sync.model.OrderRecord;
import com.example.sync.transformer.CDCToClickHouseTransformer;
import com.fasterxml.jackson.databind.ObjectMapper;
import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.common.serialization.SimpleStringSchema;
import org.apache.flink.connector.kafka.source.KafkaSource;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;

public class KafkaToClickHouseJobSimple {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.enableCheckpointing(60000);

        // Kafka Source
        KafkaSource<String> kafkaSource = KafkaSource.<String>builder()
            .setBootstrapServers("kafka:9092")
            .setTopics("orders-cdc-topic")
            .setGroupId("flink-sync-connector")
            .setValueOnlyDeserializer(new SimpleStringSchema())
            .build();

        DataStream<String> cdcStream = env
            .fromSource(kafkaSource, WatermarkStrategy.noWatermarks(), "Kafka Source");

        // ClickHouse Sink (ê°„ì†Œí™”ëœ Builder íŒ¨í„´)
        cdcStream
            .map(json -> {
                ObjectMapper mapper = new ObjectMapper();
                CDCEvent event = mapper.readValue(json, CDCEvent.class);
                return new CDCToClickHouseTransformer().map(event);
            })
            .filter(record -> record != null)
            .addSink(
                new ClickHouseSinkBuilder<OrderRecord>()
                    .withHost("clickhouse")
                    .withPort(8123)
                    .withDatabase("order_analytics")
                    .withTable("orders_realtime")
                    .withBatchSize(1000)
                    .withFlushInterval(5000)
                    .withMaxRetries(3)
                    .withFieldExtractor(record -> new Object[]{
                        record.getOrderId(),
                        record.getUserId(),
                        record.getProductName(),
                        record.getQuantity(),
                        record.getTotalPrice(),
                        record.getStatus(),
                        record.getCreatedAt(),
                        record.getUpdatedAt(),
                        record.getOperationType(),
                        record.getEventTimestamp()
                    })
                    .build()
            )
            .name("ClickHouse Sink");

        env.execute("Kafka to ClickHouse");
    }
}
```

## ğŸ”„ ë°ì´í„° ë³€í™˜ ë¡œì§

### INSERT/UPDATE ì²˜ë¦¬
```java
// CDC INSERT (op='c')
{
  "before": null,
  "after": {
    "order_id": 1001,
    "user_id": 500,
    "product_name": "Laptop",
    "quantity": 2,
    "total_price": 2000.00,
    "status": "pending"
  },
  "op": "c"
}

// ClickHouse INSERT
INSERT INTO orders_realtime VALUES (
  1001, 500, 'Laptop', 2, 2000.00, 'pending',
  '2025-01-11 10:30:00', '2025-01-11 10:30:00',
  'INSERT', 1736592600000
);
```

### DELETE ì²˜ë¦¬ (ë…¼ë¦¬ì  ì‚­ì œ)
```java
// CDC DELETE (op='d')
{
  "before": {
    "order_id": 1001,
    ...
  },
  "after": null,
  "op": "d"
}

// ClickHouse INSERT (ë…¼ë¦¬ì  ì‚­ì œ ë ˆì½”ë“œ)
INSERT INTO orders_realtime VALUES (
  1001, 500, 'Laptop', 2, 2000.00, 'pending',
  '2025-01-11 10:30:00', '2025-01-11 10:30:00',
  'DELETE', 1736596500000
);
```

## âš™ï¸ ë°°ì¹˜ ìµœì í™” (ClickHouse Native Sink)

### Batch Insert ì„¤ì •
```java
// ClickHouse Native Sink Properties
Properties clickHouseProps = new Properties();
clickHouseProps.setProperty("clickhouse.batch.size", "1000");           // 1000ê°œì”© ë°°ì¹˜
clickHouseProps.setProperty("clickhouse.batch.interval.ms", "5000");    // 5ì´ˆë§ˆë‹¤ Flush
clickHouseProps.setProperty("clickhouse.max.retries", "3");             // ìµœëŒ€ 3íšŒ ì¬ì‹œë„
clickHouseProps.setProperty("clickhouse.retry.interval.ms", "1000");    // ì¬ì‹œë„ ê°„ê²© 1ì´ˆ

// ê³ ê¸‰ ì„¤ì •
clickHouseProps.setProperty("clickhouse.write.async", "true");          // ë¹„ë™ê¸° ì“°ê¸°
clickHouseProps.setProperty("clickhouse.compression", "lz4");           // ì••ì¶• í™œì„±í™”
clickHouseProps.setProperty("clickhouse.socket.timeout", "30000");      // 30ì´ˆ íƒ€ì„ì•„ì›ƒ
```

### ClickHouse Native Sink vs JDBC ì„±ëŠ¥ ë¹„êµ
| ì„¤ì • | Native Sink | JDBC Sink | ê°œì„ ìœ¨ |
|------|-------------|-----------|--------|
| Batch ì—†ìŒ | 200 TPS | 100 TPS | **2ë°°** |
| Batch 100 | 800 TPS | 500 TPS | **1.6ë°°** |
| Batch 1000 | 2000+ TPS | 1000 TPS | **2ë°°** |
| Latency | 2-3ì´ˆ | 4-6ì´ˆ | **40% ê°ì†Œ** |

### ì„±ëŠ¥ íŠœë‹ ê°€ì´ë“œ
```properties
# ì†Œê·œëª¨ íŠ¸ë˜í”½ (100-1,000 TPS)
clickhouse.batch.size=1000
clickhouse.batch.interval.ms=5000
clickhouse.max.retries=3

# ì¤‘ê·œëª¨ íŠ¸ë˜í”½ (1,000-10,000 TPS)
clickhouse.batch.size=5000
clickhouse.batch.interval.ms=2000
clickhouse.max.retries=5
clickhouse.write.async=true

# ëŒ€ê·œëª¨ íŠ¸ë˜í”½ (10,000+ TPS)
clickhouse.batch.size=10000
clickhouse.batch.interval.ms=1000
clickhouse.max.retries=10
clickhouse.write.async=true
clickhouse.compression=lz4
```

## ğŸ” ëª¨ë‹ˆí„°ë§

### Flink Web UI í™•ì¸
```
http://localhost:8081

í™•ì¸ í•­ëª©:
- Job Status: RUNNING
- Records In (Kafka): ì´ˆë‹¹ ì†Œë¹„ ë ˆì½”ë“œ ìˆ˜
- Records Out (ClickHouse): ì´ˆë‹¹ ì‚½ì… ë ˆì½”ë“œ ìˆ˜
- Backpressure: ì—­ì•• ìƒíƒœ í™•ì¸
- Checkpoint: ì„±ê³µë¥  ë° ì£¼ê¸°
```

### Kafka Consumer Lag í™•ì¸
```bash
docker exec -it kafka kafka-consumer-groups --describe \
  --bootstrap-server localhost:9092 \
  --group flink-sync-connector

# ì¶œë ¥ ì˜ˆì‹œ:
# TOPIC              PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG
# orders-cdc-topic   0          1500            1500            0
# orders-cdc-topic   1          1500            1500            0
# orders-cdc-topic   2          1500            1500            0
```

### ClickHouse ë°ì´í„° í™•ì¸
```sql
-- ë ˆì½”ë“œ ìˆ˜ í™•ì¸
SELECT COUNT(*) FROM orders_realtime;

-- ìµœê·¼ ì‚½ì…ëœ ë ˆì½”ë“œ
SELECT * FROM orders_realtime
ORDER BY event_timestamp DESC
LIMIT 10;

-- ì‘ì—… ìœ í˜•ë³„ í†µê³„
SELECT operation_type, COUNT(*) AS cnt
FROM orders_realtime
GROUP BY operation_type;
```

## ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤

### 1. End-to-End í…ŒìŠ¤íŠ¸
```bash
# 1. MySQLì— ë°ì´í„° ì‚½ì…
docker exec -it mysql mysql -u root -p
USE order_db;
INSERT INTO orders (user_id, product_name, quantity, total_price, status)
VALUES (100, 'Test Product', 1, 50.00, 'pending');

# 2. Kafka Topic í™•ì¸ (1-2ì´ˆ í›„)
docker exec -it kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic orders-cdc-topic \
  --max-messages 1

# 3. ClickHouse í™•ì¸ (3-7ì´ˆ í›„)
docker exec -it clickhouse-server clickhouse-client \
  --query "SELECT * FROM orders_realtime ORDER BY event_timestamp DESC LIMIT 10"
```

### 2. ëŒ€ëŸ‰ ë°ì´í„° í…ŒìŠ¤íŠ¸
```bash
# 100ê±´ ì‚½ì…
for i in {1..100}; do
  docker exec -it mysql mysql -u root -p order_db \
    -e "INSERT INTO orders (user_id, product_name, quantity, total_price) VALUES ($i, 'Product $i', 1, 100.00);"
done

# ClickHouse ì¹´ìš´íŠ¸ í™•ì¸
docker exec -it clickhouse-server clickhouse-client \
  --query "SELECT COUNT(*) FROM orders_realtime"
```

### 3. ì¥ì•  ë³µêµ¬ í…ŒìŠ¤íŠ¸
```bash
# Flink Job ì¬ì‹œì‘
docker restart flink-jobmanager

# Checkpointì—ì„œ ë³µêµ¬ í™•ì¸
# - Consumer Lag í™•ì¸
# - ë°ì´í„° ì •í•©ì„± í™•ì¸ (MySQL vs ClickHouse)
```

## ğŸš¨ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: ClickHouse ì—°ê²° ì‹¤íŒ¨
```bash
# ì—°ê²° í…ŒìŠ¤íŠ¸
docker exec -it clickhouse-server clickhouse-client --query "SELECT 1"

# ì›ì¸:
# - ClickHouse ì»¨í…Œì´ë„ˆ ë¯¸ì‹¤í–‰
# - JDBC URL ì˜¤ë¥˜
# - ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ

# í•´ê²°:
docker ps | grep clickhouse
docker logs clickhouse-server
```

### ë¬¸ì œ 2: Batch Insert ì‹¤íŒ¨
```bash
# Flink Job ë¡œê·¸ í™•ì¸
docker logs flink-taskmanager | grep ERROR

# ì¼ë°˜ì ì¸ ì›ì¸:
# - ClickHouse í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ë¶ˆì¼ì¹˜
# - Null ê°’ ì²˜ë¦¬ ì˜¤ë¥˜
# - íƒ€ì„ìŠ¤íƒ¬í”„ í¬ë§· ì˜¤ë¥˜

# í•´ê²°: ìŠ¤í‚¤ë§ˆ í™•ì¸
docker exec -it clickhouse-server clickhouse-client \
  --query "DESCRIBE TABLE orders_realtime"
```

### ë¬¸ì œ 3: Consumer Lag ì¦ê°€
```bash
# Lag í™•ì¸
docker exec -it kafka kafka-consumer-groups --describe \
  --bootstrap-server localhost:9092 \
  --group flink-sync-connector

# ì›ì¸:
# - ClickHouse INSERT ì†ë„ < Kafka Produce ì†ë„
# - Batch í¬ê¸°ê°€ ë„ˆë¬´ ì‘ìŒ
# - TaskManager ë¦¬ì†ŒìŠ¤ ë¶€ì¡±

# í•´ê²°:
# 1. Batch í¬ê¸° ì¦ê°€ (1000 â†’ 5000)
# 2. Batch Interval ì¦ê°€ (5ì´ˆ â†’ 10ì´ˆ)
# 3. TaskManager ë³‘ë ¬ë„ ì¦ê°€
```

### ë¬¸ì œ 4: ë°ì´í„° ì •í•©ì„± ì˜¤ë¥˜
```bash
# MySQL vs ClickHouse ì¹´ìš´íŠ¸ ë¹„êµ
# MySQL
docker exec -it mysql mysql -u root -p order_db \
  -e "SELECT COUNT(*) FROM orders"

# ClickHouse (ë…¼ë¦¬ì  ì‚­ì œ ì œì™¸)
docker exec -it clickhouse-server clickhouse-client \
  --query "SELECT COUNT(*) FROM orders_realtime WHERE operation_type != 'DELETE'"

# ë¶ˆì¼ì¹˜ ì‹œ:
# - Flink Checkpoint í™•ì¸
# - Kafka Consumer Offset ë¦¬ì…‹
# - ë°ì´í„° ì¬ë™ê¸°í™”
```

## ğŸ”’ ì„±ëŠ¥ ìµœì í™” (ClickHouse Native Sink)

### 1. ClickHouse Sink ì„¤ì • íŠœë‹
```java
Properties clickHouseProps = new Properties();

// ì†Œê·œëª¨ íŠ¸ë˜í”½ (100-1,000 TPS) - MVP í™˜ê²½
clickHouseProps.setProperty("clickhouse.batch.size", "1000");
clickHouseProps.setProperty("clickhouse.batch.interval.ms", "5000");
clickHouseProps.setProperty("clickhouse.write.async", "false");

// ì¤‘ê·œëª¨ íŠ¸ë˜í”½ (1,000-10,000 TPS)
clickHouseProps.setProperty("clickhouse.batch.size", "5000");
clickHouseProps.setProperty("clickhouse.batch.interval.ms", "2000");
clickHouseProps.setProperty("clickhouse.write.async", "true");
clickHouseProps.setProperty("clickhouse.compression", "lz4");

// ëŒ€ê·œëª¨ íŠ¸ë˜í”½ (10,000+ TPS)
clickHouseProps.setProperty("clickhouse.batch.size", "10000");
clickHouseProps.setProperty("clickhouse.batch.interval.ms", "1000");
clickHouseProps.setProperty("clickhouse.write.async", "true");
clickHouseProps.setProperty("clickhouse.compression", "zstd");
clickHouseProps.setProperty("clickhouse.max.parallel.requests", "5");
```

### 2. Parallelism ì¡°ì •
```java
// TaskManager ë³‘ë ¬ë„
env.setParallelism(4);

// ClickHouse Sink ë³‘ë ¬ë„ (Partition ìˆ˜ì™€ ë™ì¼í•˜ê²Œ ì„¤ì •)
orderStream
    .addSink(ClickHouseSink.sink(sinkFunction))
    .setParallelism(3)  // Kafka Partition ìˆ˜ = 3
    .name("ClickHouse Native Sink");
```

### 3. Checkpoint ìµœì í™”
```java
// Checkpoint ê°„ê²©
env.enableCheckpointing(60000);  // 1ë¶„

// Checkpoint ëª¨ë“œ (Exactly-Once)
env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);

// Checkpoint ì €ì¥ì†Œ
env.getCheckpointConfig().setCheckpointStorage("hdfs://namenode:9000/flink-checkpoints");

// Checkpoint íƒ€ì„ì•„ì›ƒ
env.getCheckpointConfig().setCheckpointTimeout(600000);  // 10ë¶„
```

### 4. ClickHouse í…Œì´ë¸” ìµœì í™”
```sql
-- ReplacingMergeTreeë¡œ ì¤‘ë³µ ì œê±°
CREATE TABLE orders_realtime (
    ...
) ENGINE = ReplacingMergeTree(event_timestamp)
PARTITION BY toYYYYMM(created_at)
ORDER BY (order_id, event_timestamp)
SETTINGS index_granularity = 8192;

-- ì£¼ê¸°ì  OPTIMIZE ì‹¤í–‰ (ì¤‘ë³µ ì œê±°)
OPTIMIZE TABLE orders_realtime FINAL;
```

### 5. ë„¤íŠ¸ì›Œí¬ ìµœì í™”
```java
// ì—°ê²° í’€ ì„¤ì •
clickHouseProps.setProperty("clickhouse.socket.timeout", "30000");
clickHouseProps.setProperty("clickhouse.connection.timeout", "10000");
clickHouseProps.setProperty("clickhouse.max.connections.per.host", "10");
clickHouseProps.setProperty("clickhouse.socket.keepalive", "true");
```

## ğŸ“š ë‹¤ìŒ ë‹¨ê³„
- [ClickHouse ìŠ¤í‚¤ë§ˆ ì„¤ê³„](./05-clickhouse-schema.md) - ì‹¤ì‹œê°„ ë¶„ì„ í…Œì´ë¸” êµ¬ì¡°
- [MySQL ìŠ¤í‚¤ë§ˆ ì„¤ì •](../order-service/mysql-schema.md) - CDC ì†ŒìŠ¤ í…Œì´ë¸”
